# Studio Ghibli ETL Pipeline

A comprehensive ETL (Extract, Transform, Load) pipeline that extracts data from the Studio Ghibli API, transforms it using pandas, and loads it into MongoDB for analysis and storage. The project includes both standalone Python ETL scripts and Apache Airflow DAGs for orchestrated data pipelines.

**Base URL:** https://ghibliapi.vercel.app/

## âœ¨ Features

- **Standalone ETL Script**: Direct Python execution for immediate data processing
- **Apache Airflow Integration**: Orchestrated workflows using Astro CLI
- **Data Validation**: Comprehensive logging and error handling
- **Multiple Data Sources**: Films, people, locations, species, and vehicles
- **MongoDB Storage**: Scalable NoSQL database integration

## ğŸ“‹ Overview

This project demonstrates a complete ETL workflow by:
- Extracting movie, character, and location data from the Studio Ghibli API
- Cleaning and transforming the data using pandas
- Storing the processed data in MongoDB for future analysis

## ğŸ”„ ETL Flow

1. **Extract**: Fetch data from Studio Ghibli API endpoints using `requests`
2. **Transform**: Clean, normalize, and structure data using `pandas`
3. **Load**: Insert transformed data into MongoDB collections using `pymongo`

## ğŸ“š API Endpoints

The project extracts data from the following endpoints:
- `/films` - Studio Ghibli movies
- `/people` - Characters from the movies
- `/locations` - Locations featured in the films
- `/species` - Different species in the Ghibli universe
- `/vehicles` - Vehicles used in the movies

## ğŸš€ Usage

### Standalone Execution

Activate your virtual environment and run the ETL pipeline:
```bash
source new_env/bin/activate  # Activate virtual environment
python Studio_Ghibli_ETL.py
```

### Airflow/Astro Execution

1. Start the Astro environment:
   ```bash
   cd astroProject
   astro dev start
   ```

2. Access Airflow UI at `http://localhost:8080`
   - Username: `admin`
   - Password: `admin`

3. Trigger the Studio Ghibli ETL DAG from the Airflow UI

### Monitoring

Check the ETL execution logs:
```bash
# View the log file
tail -f ETL_log.log
```

## ğŸ“ Project Structure

```
Studio_Ghibli_ETL/
â”œâ”€â”€ Studio_Ghibli_ETL.py    # Main ETL script
â”œâ”€â”€ ETL_log.log             # Execution logs
â”œâ”€â”€ species_raw.json        # Raw species data cache
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ new_env/                # Python virtual environment
â”œâ”€â”€ astroProject/           # Apache Airflow project (Astro CLI)
â”‚   â”œâ”€â”€ dags/               # Airflow DAGs
â”‚   â”‚   â”œâ”€â”€ Studio_Ghibli_ETL.py  # Airflow version of ETL
â”‚   â”‚   â”œâ”€â”€ exampledag.py   # Example DAG
â”œâ”€â”€ raw_json/               # Raw API response cache
â”‚   â”œâ”€â”€ films_raw.json
|   â”œâ”€â”€ ...  
â”‚   â””â”€â”€ vehicles_raw.json
â””â”€â”€ sample_json/            # Sample data for testing
    â”œâ”€â”€ films_sample.json
    â”œâ”€â”€ ...  
    â””â”€â”€ vehicles_sample.json

```

## ğŸ“Š Expected Output

After successful execution, your MongoDB will contain collections:
- `films` - Movie information and metadata
- `characters` - Character details and relationships
- `locations` - Location data with geographical information
- `species` - Species information and characteristics
- `vehicles` - Vehicle data and specifications

### Log Files
- `ETL_log.log` - Contains detailed execution logs, errors, and processing statistics
- Raw JSON files cached in `raw_json/` directory for offline processing

## ğŸ“ Development Notes

- The project uses a virtual environment (`new_env/`) for dependency isolation
- Raw data is cached locally to reduce API calls during development
- Both standalone and Airflow execution methods are supported
- Comprehensive logging is implemented for debugging and monitoring

## ğŸ“„ License

This project is for educational purposes and uses the public Studio Ghibli API.